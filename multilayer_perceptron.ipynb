{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron - Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook tem como intenção implementar um multilayer perceptron para classificar o Iris Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para execução do código é necessária a importação dos pacotes random e pprint, como no bloco executável abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso deseje obter os mesmos resultados sempre que executar o código, descomente o código abaixo e o execute, para que os valores aleatórios sejam gerados a partir de uma mesma semente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo estão as funções auxiliares utilizadas para leitura de arquivo, conversão de valores, calcúlos matriciais, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file\n",
    "def read_data_set(data_set):\n",
    "    with open(data_set) as data_file:\n",
    "        data_set = data_file.read().split(\"\\n\")\n",
    "        for i in range(len(data_set)):\n",
    "            data_set[i] = data_set[i].split(\",\")\n",
    "\n",
    "    return data_set\n",
    "\n",
    "\n",
    "# Change classification to int values\n",
    "def get_classification_value(classification):\n",
    "    if(classification == \"Iris-setosa\"):\n",
    "        return 0\n",
    "    if(classification == \"Iris-versicolor\"):\n",
    "        return 1\n",
    "    if(classification == \"Iris-virginica\"):\n",
    "        return 2\n",
    "\n",
    "\n",
    "# Convert string list to float list\n",
    "def change_string_to_float(string_list):\n",
    "    float_list = []\n",
    "    for i in range(len(string_list)):\n",
    "        float_list.append(float(string_list[i]))\n",
    "    return float_list\n",
    "\n",
    "\n",
    "# Matrix multiplication (for Testing)\n",
    "def matrix_mul_bias(A, B, bias):\n",
    "    C = []\n",
    "    for i in range(len(A)):\n",
    "        C.append([])\n",
    "        for j in range(len(B[0])):\n",
    "            C[i].append(0)\n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(B[0])):\n",
    "            for k in range(len(B)):\n",
    "                C[i][j] += A[i][k] * B[k][j]\n",
    "            C[i][j] += bias[j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "# Vector (A) x matrix (B) multiplication\n",
    "def vec_mat_bias(A, B, bias):\n",
    "    C = []\n",
    "    for i in range(len(B[0])):\n",
    "        C.append(0)\n",
    "    \n",
    "    for j in range(len(B[0])):\n",
    "        for k in range(len(B)):\n",
    "            C[j] += A[k] * B[k][j]\n",
    "        C[j] += bias[j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "# Matrix (A) x vector (B) multipilicatoin (for backprop)\n",
    "def mat_vec(A, B): \n",
    "    C = []\n",
    "    for i in range(len(A)):\n",
    "        C.append(0)\n",
    "    \n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(B)):\n",
    "            C[i] += A[i][j] * B[j]\n",
    "    \n",
    "    return C\n",
    "\n",
    "\n",
    "# derivation of sigmoid (for backprop)\n",
    "def sigmoid(A, deriv=False):\n",
    "    if deriv: \n",
    "        for i in range(len(A)):\n",
    "            A[i] = A[i] * (1 - A[i])\n",
    "    else:\n",
    "        for i in range(len(A)):\n",
    "            A[i] = 1 / (1 + math.exp(-A[i]))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função principal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função principal se inicia lendo o arquivo chamado, por padrão, de \"iris.txt\", contendo o Iris Dataset. Após isso, ele converte suas entradas para valores flutuantes, o divide em dois conjuntos, um de treino e outro de teste e, por fim, separa os resultados dos vetores de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = read_data_set(\"iris.txt\")\n",
    "\n",
    "# Change string value to numeric\n",
    "for line in data_set:\n",
    "    line[4] = get_classification_value(line[4])\n",
    "    line[:4] = change_string_to_float(line)\n",
    "\n",
    "\n",
    "# Create a train and a test data)\n",
    "data_train = data_set\n",
    "data_test = []\n",
    "\n",
    "num_max = 15\n",
    "\n",
    "num_max_0 = 0\n",
    "num_max_1 = 0\n",
    "num_max_2 = 0\n",
    "\n",
    "for t in data_set:\n",
    "    if(num_max_0 < num_max and t[4] == 0):\n",
    "        num_max_0 += 1\n",
    "        data_test.append(t)\n",
    "    if(num_max_1 < num_max and t[4] == 1):\n",
    "        num_max_1 += 1\n",
    "        data_test.append(t)\n",
    "    if(num_max_2 < num_max and t[4] == 2):\n",
    "        num_max_2 += 1\n",
    "        data_test.append(t)\n",
    "\n",
    "\n",
    "train_set = []\n",
    "train_result = []\n",
    "for data in data_train:\n",
    "    # split the entrance and the result\n",
    "    train_set.append(data[:4])\n",
    "    train_result.append(data[4])\n",
    "\n",
    "test_set = []\n",
    "test_result = []\n",
    "for data in data_test:\n",
    "    # split the entrance and the result\n",
    "    test_set.append(data[:4])\n",
    "    test_result.append(data[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feito isso, ele define a taxa de aprendizado, a quantidade de épocas e quantos neurônios cada camada terá. Após isso o programa gera os vetores de pesos e os vetores bias, inicializando seus valores aleatóriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter\n",
    "alpha = 0.01\n",
    "epoch = 700\n",
    "neurons = [4, 6, 3] # number of neurons each layer\n",
    "\n",
    "\n",
    "# Initiate weight and bias with 0 value\n",
    "weights = []\n",
    "for i in range(len(neurons) - 1):\n",
    "    weights.append([])\n",
    "    for j in range(neurons[i]):\n",
    "        weights[i].append([])\n",
    "        for k in range(neurons[i + 1]):\n",
    "            weights[i][j].append(0)\n",
    "\n",
    "weight = []\n",
    "for i in range(len(weights[0])):\n",
    "    weight.append([])\n",
    "    for j in range(len(weights[0][i])):\n",
    "        weight[i].append(weights[0][i][j])\n",
    "\n",
    "weight_2 = []\n",
    "for i in range(len(weights[1])):\n",
    "    weight_2.append([])\n",
    "    for j in range(len(weights[1][i])):\n",
    "        weight_2[i].append(weights[1][i][j])\n",
    "\n",
    "\n",
    "bias_list = []\n",
    "for i in range(1, len(neurons)):\n",
    "    bias_list.append([])\n",
    "    for j in range(neurons[i]):\n",
    "        bias_list[i-1].append(0)\n",
    "\n",
    "bias = []\n",
    "for i in range(len(bias_list[0])):\n",
    "    bias.append(bias_list[0][i])\n",
    "\n",
    "bias_2 = []\n",
    "for i in range(len(bias_list[1])):\n",
    "    bias_2.append(bias_list[1][i])\n",
    "\n",
    "# Initiate weight with random between -1.0 ... 1.0\n",
    "for i in range(neurons[0]):\n",
    "    for j in range(neurons[1]):\n",
    "        weight[i][j] = 2 * random.random() - 1\n",
    "\n",
    "for i in range(neurons[1]):\n",
    "    for j in range(neurons[2]):\n",
    "        weight_2[i][j] = 2 * random.random() - 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os valores inicializados, é feito o treinamento. Para cada item do conjunto de treino é feito o Foward Propagation (FP). Com os resultados do FP, calcula-se o erro total da rede. Após isso, é feito o Backward Propagation (BP), atualizando-se os valores dos pesos e dos bias para cada camada, começando pelas últimas. Com isso feito, imprime-se o valor do erro e o processo é repetido até que se tenha completado o número de épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro da epoca  0 :  0.36269889651659154\n",
      "Erro da epoca  100 :  0.12903855948861906\n",
      "Erro da epoca  200 :  0.0632752739116513\n",
      "Erro da epoca  300 :  0.04148430869867832\n",
      "Erro da epoca  400 :  0.03227574851071418\n",
      "Erro da epoca  500 :  0.027355160049427632\n",
      "Erro da epoca  600 :  0.024326805640054797\n",
      "Erro da ultima epoca:  0.022310269294145847\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    cost_total = 0\n",
    "    for idx, data_list in enumerate(train_set): # Update for each data; SGD\n",
    "\n",
    "\n",
    "        # Forward propagation\n",
    "        h_1 = vec_mat_bias(data_list, weight, bias)\n",
    "        X_1 = sigmoid(h_1)\n",
    "        h_2 = vec_mat_bias(X_1, weight_2, bias_2)\n",
    "        X_2 = sigmoid(h_2)\n",
    "\n",
    "\n",
    "\n",
    "        # Convert to One-hot target\n",
    "        target = [0] * neurons[-1]\n",
    "        target[int(train_result[idx])] = 1\n",
    "\n",
    "\n",
    "        # Cost function, Square Root Eror\n",
    "        eror = 0\n",
    "        for i in range(neurons[-1]):\n",
    "            eror +=  0.5 * (target[i] - X_2[i]) ** 2 \n",
    "        cost_total += eror\n",
    "\n",
    "        # Backward propagation\n",
    "        delta_2 = []\n",
    "        for j in range(neurons[2]):\n",
    "            delta_2.append(-1 * (target[j]-X_2[j]) * X_2[j] * (1-X_2[j]))\n",
    "\n",
    "\n",
    "        for i in range(neurons[1]):\n",
    "            for j in range(neurons[2]):\n",
    "                weight_2[i][j] -= alpha * (delta_2[j] * X_1[i])\n",
    "                bias_2[j] -= alpha * delta_2[j]\n",
    "\n",
    "        delta_1 = mat_vec(weight_2, delta_2)\n",
    "        for j in range(neurons[1]):\n",
    "            delta_1[j] = delta_1[j] * (X_1[j] * (1-X_1[j]))\n",
    "\n",
    "\n",
    "        # Update weight and bias (layer 1)\n",
    "        delta_1 = mat_vec(weight_2, delta_2)\n",
    "        for j in range(neurons[1]):\n",
    "            delta_1[j] = delta_1[j] * (X_1[j] * (1-X_1[j]))\n",
    "\n",
    "        for i in range(neurons[0]):\n",
    "            for j in range(neurons[1]):\n",
    "                weight[i][j] -=  alpha * (delta_1[j] * data_list[i])\n",
    "                bias[j] -= alpha * delta_1[j]\n",
    "\n",
    "\n",
    "    cost_total /= len(train_set)\n",
    "    if(e % 100 == 0):\n",
    "        print(\"Erro da epoca \", e, \": \", cost_total)\n",
    "    elif(e == epoch - 1):\n",
    "        print(\"Erro da ultima epoca: \", cost_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, é realizado o teste para cada um dos elementos no conjunto de testes. Os resultados são impressos na tela na seguinte ordem: os resultados esperados (vindos do Iris Dataset),  a predição (feita pela rede) e a taxa de acerto (acc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado esperado:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Predição: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2]\n",
      "97.77777777777777 %\n"
     ]
    }
   ],
   "source": [
    "res = matrix_mul_bias(test_set, weight, bias)\n",
    "res_2 = matrix_mul_bias(res, weight_2, bias_2)\n",
    "\n",
    "# Get prediction\n",
    "preds = []\n",
    "for r in res_2:\n",
    "    preds.append(max(enumerate(r), key=lambda x:x[1])[0])\n",
    "\n",
    "for i in range(len(test_result)):\n",
    "    test_result[i] = int(test_result[i])\n",
    "# Print prediction\n",
    "print(\"Resultado esperado: \", test_result)\n",
    "print(\"Predição:\", preds)\n",
    "\n",
    "\n",
    "# Calculate accuration\n",
    "acc = 0.0\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == int(test_result[i]):\n",
    "        acc += 1\n",
    "print(acc / len(preds) * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
